{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comments ü§¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω ¬´–í–∏–∫–∏—à–æ–ø¬ª –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é.\n",
    "\n",
    "**–¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞:** –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ**<a id='toc0_'></a>    \n",
    "1. [ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞   ](#toc1_)    \n",
    "1.1. [–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏   ](#toc1_1_)    \n",
    "1.2. [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è   ](#toc1_2_)    \n",
    "1.3. [–°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏   ](#toc1_3_)    \n",
    "2. [–î–∞–Ω–Ω—ã–µ   ](#toc2_)    \n",
    "2.1. [–ó–∞–≥—Ä—É–∑–∫–∞   ](#toc2_1_)    \n",
    "2.2. [–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö   ](#toc2_2_)    \n",
    "2.3. [–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤   ](#toc2_3_)    \n",
    "2.4. [–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤   ](#toc2_4_)    \n",
    "2.5. [–ò–∑—É—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞   ](#toc2_5_)    \n",
    "2.6. [–í—ã–≤–æ–¥—ã –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É   ](#toc2_6_)    \n",
    "3. [–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞   ](#toc3_)    \n",
    "4. [–ú–æ–¥–µ–ª–∏   ](#toc4_)    \n",
    "4.1. [–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö   ](#toc4_1_)    \n",
    "4.2. [–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞  ](#toc4_2_)    \n",
    "4.3. [–ú–æ–¥–µ–ª—å `LogisticRegression`  ](#toc4_3_)    \n",
    "4.4. [–ú–æ–¥–µ–ª—å `RandomForestClassifier` ](#toc4_4_)    \n",
    "4.5. [–ú–æ–¥–µ–ª—å `LGBMClassifier` ](#toc4_5_)    \n",
    "4.6. [–ú–æ–¥–µ–ª—å `CatBoostClassifier` ](#toc4_6_)    \n",
    "5. [–†–µ–∑—É–ª—å—Ç–∞—Ç—ã ](#toc5_)    \n",
    "5.1. [–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ ](#toc5_1_)    \n",
    "5.2. [–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ ](#toc5_2_)    \n",
    "6. [–û–±—â–∏–π –≤—ã–≤–æ–¥ ](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1_'></a> –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. <a id='toc1_1_'></a>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q catboost==1.2.7\n",
    "%pip install -q dill==0.3.8\n",
    "%pip install -q hyperopt==0.2.7\n",
    "%pip install -q lightgbm==4.5.0\n",
    "%pip install -q matplotlib==3.8.4\n",
    "%pip install -q nltk==3.9.1\n",
    "%pip install -q numpy==1.26.4\n",
    "%pip install -q pandas==2.2.3\n",
    "%pip install -q prettytable==3.12.0\n",
    "%pip install -q scikit-learn==1.5.2\n",
    "%pip install -q termcolor==2.5.0\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import dill\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from prettytable import PrettyTable\n",
    "from termcolor import colored\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     StratifiedKFold,\n",
    "                                     cross_val_score)\n",
    "from sklearn.metrics import (f1_score,\n",
    "                             classification_report,\n",
    "                             ConfusionMatrixDisplay)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from hyperopt import (hp,\n",
    "                      fmin,\n",
    "                      tpe,\n",
    "                      Trials,\n",
    "                      STATUS_OK,\n",
    "                      STATUS_FAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. <a id='toc1_2_'></a>–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 27\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. <a id='toc1_3_'></a>–°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(paths: list[str], **kwargs) -> pd.DataFrame:\n",
    "    for _path in paths:\n",
    "        if not exists(_path) and not _path.startswith('http'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(_path, **kwargs)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if df is None:\n",
    "            continue\n",
    "\n",
    "        return df\n",
    "\n",
    "    raise FileNotFoundError('No paths are valid for correct csv file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df: pd.DataFrame) -> None:\n",
    "    duplicates_count = df.duplicated().sum()\n",
    "\n",
    "    if duplicates_count == 0:\n",
    "        print(colored('–ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.', 'green'))\n",
    "        return\n",
    "\n",
    "    duplicates_part = duplicates_count / len(df)\n",
    "    print(colored(f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ({duplicates_part:.2%})', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nans(df: pd.DataFrame) -> None:\n",
    "    if df.isna().sum().sum() == 0:\n",
    "        print(colored('–ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.', 'green'))\n",
    "        return\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Feature', 'Missing values count']\n",
    "\n",
    "    missing_info = df.isna().sum().sort_values()\n",
    "    cols = missing_info.index.to_list()\n",
    "    for col in cols:\n",
    "        count = missing_info[col]\n",
    "        color = 'green' if count == 0 else 'red'\n",
    "        s = f'{count} ({count / len(df):.2%})'\n",
    "        table.add_row([col, colored(s, color)])\n",
    "\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_counts(series: pd.Series) -> None:\n",
    "    data = pd.DataFrame()\n",
    "    data['count'] = series.value_counts()\n",
    "    data['part'] = round(data['count'] / len(series), 4)\n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>–î–∞–Ω–Ω—ã–µ    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. <a id='toc2_1_'></a>–ó–∞–≥—Ä—É–∑–∫–∞    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe([\n",
    "    './data/toxic_comments.csv',\n",
    "    'datasets/toxic_comments.csv'\n",
    "], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <a id='toc2_2_'></a>–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. <a id='toc2_3_'></a>–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. <a id='toc2_4_'></a>–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. <a id='toc2_5_'></a>–ò–∑—É—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_value_counts(df['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_counts = df['toxic'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(toxic_counts,\n",
    "        labels=['Not Toxic (0)', 'Toxic (1)'],\n",
    "        colors=['lightgreen', 'lightcoral'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–∏–º –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–ª—å—à–æ–π –¥–∏–∑–±–∞–ª–∞–Ω—Å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. <a id='toc2_6_'></a>–í—ã–≤–æ–¥—ã –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–¥ –Ω–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç –æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö —Å –æ—Ü–µ–Ω–∫–æ–π –∏—Ö —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏. –ü—Ä–æ–ø—É—Å–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–∞–∫–∂–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∫—Ä–∞–π–Ω–µ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ. –ó–∞–ø–∏—Å–µ–π —Å —Ç–∞—Ä–≥–µ—Ç–æ–º `0` (not toxic) –ø–æ—á—Ç–∏ –≤ 9 —Ä–∞–∑ –±–æ–ª—å—à–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = nltk.word_tokenize(cleaned_text)\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>–ú–æ–¥–µ–ª–∏    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. <a id='toc4_1_'></a>–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö    [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=TEST_SIZE,\n",
    "                                                    random_state=RANDOM_STATE,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. <a id='toc4_2_'></a>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞   [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_objective(estimator,\n",
    "                  X_train: pd.DataFrame,\n",
    "                  y_train: pd.Series):\n",
    "    def objective(params: dict) -> float:\n",
    "        \"\"\"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ç–µ–∫—É—â–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
    "\n",
    "        Args:\n",
    "            estimator: –ø–∞–π–ø–ª–∞–π–Ω —Å –º–æ–¥–µ–ª—å—é –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ –º–æ–¥–µ–ª—å\n",
    "            params (dict): –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "            X_train (pd.DataFrame): –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (features)\n",
    "            y_train (pd.Series): —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ (target)\n",
    "\n",
    "        Returns:\n",
    "            dict: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫–∏, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Å—Ç–∞—Ç—É—Å–æ–º.\n",
    "        \"\"\"\n",
    "        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ, –ø—Ä–∏–≤–æ–¥–∏–º –∏—Ö –∫ –Ω—É–∂–Ω–æ–º—É —Ç–∏–ø—É\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, float) and value % 1 == 0:\n",
    "                params[key] = int(value)\n",
    "\n",
    "        estimator.set_params(**params)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "        try:\n",
    "            score = cross_val_score(estimator=estimator,\n",
    "                                    X=X_train,\n",
    "                                    y=y_train,\n",
    "                                    scoring='f1',\n",
    "                                    cv=skf,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "            return {\n",
    "                'loss': -score.mean(),\n",
    "                'params': params,\n",
    "                'status': STATUS_OK\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {'status': STATUS_FAIL}\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(estimator,\n",
    "                      param_space: dict,\n",
    "                      X_train: pd.DataFrame,\n",
    "                      y_train: pd.Series,\n",
    "                      max_evals: int = 1000\n",
    "                      ) -> tuple[dict, float]:\n",
    "    objective = get_objective(estimator, X_train, y_train)\n",
    "    trials = Trials()\n",
    "\n",
    "    fmin(\n",
    "        fn=objective,\n",
    "        space=param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(RANDOM_STATE),\n",
    "        show_progressbar=True\n",
    "    )\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    best_params = trials.best_trial['result']['params']\n",
    "    best_score = abs(trials.best_trial['result']['loss'])\n",
    "    print(f'Finish with best F1 = {best_score:.4f}')\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. <a id='toc4_3_'></a>–ú–æ–¥–µ–ª—å `LogisticRegression`   [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('model', LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_space = {\n",
    "    'tfidf__max_df':       hp.uniform('tfidf__max_df', 0.7, 1.0),                      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__min_df':       hp.uniform('tfidf__min_df', 0.0, 0.3),                      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__ngram_range':  hp.choice('tfidf__ngram_range', [(1, 1), (1, 2), (1, 3)]),  # –î–∏–∞–ø–∞–∑–æ–Ω n-–≥—Ä–∞–º–º\n",
    "    'tfidf__max_features': hp.quniform('tfidf__max_features', 1000, 10000, 100),       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ü–µ–ª—ã–µ —á–∏—Å–ª–∞)\n",
    "\n",
    "    'model__C':            hp.loguniform('model__C', -5, 2),                           # –û–±—Ä–∞—Ç–Ω–∞—è —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (log-scale)\n",
    "    'model__penalty':      hp.choice('model__penalty', ['l2', 'l1']),                  # –¢–∏–ø —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (L2 –∏–ª–∏ L1)\n",
    "    'model__solver':       hp.choice('model__solver', ['liblinear', 'saga']),          # –ê–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "    'model__class_weight': hp.choice('model__class_weight', [None, 'balanced']),       # –í–µ—Å –∫–ª–∞—Å—Å–æ–≤ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–∞–Ω—Å –∏–ª–∏ –Ω–µ—Ç)\n",
    "    'model__max_iter':     hp.quniform('model__max_iter', 100, 1000, 100)              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π (—Ü–µ–ª—ã–µ —á–∏—Å–ª–∞)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_lr, best_score_lr = get_model_results(lr_pipeline, param_space, X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lr = {\n",
    "    'model__C': 0.07580860544027326,\n",
    "    'model__class_weight': 'balanced',\n",
    "    'model__max_iter': 400,\n",
    "    'model__penalty': 'l2',\n",
    "    'model__solver': 'saga',\n",
    "    'tfidf__max_df': 0.9800160986993187,\n",
    "    'tfidf__max_features': 6100,\n",
    "    'tfidf__min_df': 0.0005461737876119033,\n",
    "    'tfidf__ngram_range': (1, 2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lr = 0.6813113185647047"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. <a id='toc4_4_'></a>–ú–æ–¥–µ–ª—å `RandomForestClassifier`  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'tfidf__max_features':       hp.choice('tfidf__max_features', [None, 1000, 5000, 10000]),       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    'tfidf__ngram_range':        hp.choice('tfidf__ngram_range', [(1, 1), (1, 2), (1, 3)]),         # –î–∏–∞–ø–∞–∑–æ–Ω n-–≥—Ä–∞–º–º –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\n",
    "    'tfidf__min_df':             hp.choice('tfidf__min_df', [1, 2, 5, 10]),                         # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__max_df':             hp.uniform('tfidf__max_df', 0.7, 1.0),                             # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__use_idf':            hp.choice('tfidf__use_idf', [True, False]),                        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "    'tfidf__smooth_idf':         hp.choice('tfidf__smooth_idf', [True, False]),                     # –°–≥–ª–∞–∂–∏–≤–∞—Ç—å IDF –≤–µ—Å–∞\n",
    "    'tfidf__sublinear_tf':       hp.choice('tfidf__sublinear_tf', [True, False]),                   # –ü—Ä–∏–º–µ–Ω—è—Ç—å —Å—É–±–ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ TF\n",
    "\n",
    "    'model__n_estimators':       hp.choice('model__n_estimators', [50, 100, 200, 500]),             # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É\n",
    "    'model__criterion':          hp.choice('model__criterion', ['gini', 'entropy']),                # –ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è\n",
    "    'model__max_depth':          hp.choice('model__max_depth', [None, 10, 20, 30, 50]),             # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞\n",
    "    'model__min_samples_split':  hp.choice('model__min_samples_split', [2, 5, 10]),                 # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è\n",
    "    'model__min_samples_leaf':   hp.choice('model__min_samples_leaf', [1, 2, 4]),                   # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ\n",
    "    'model__bootstrap':          hp.choice('model__bootstrap', [True, False]),                      # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—É—Ç—Å—Ç—Ä—ç–ø –≤—ã–±–æ—Ä–∫–∏\n",
    "    'model__class_weight':       hp.choice('model__class_weight', [None, 'balanced']),              # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_rfc, best_score_rfc = get_model_results(rfc_pipeline, param_space, X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rfc = {\n",
    "    'model__bootstrap': False,\n",
    "    'model__class_weight': None,\n",
    "    'model__criterion': 'gini',\n",
    "    'model__max_depth': None,\n",
    "    'model__min_samples_leaf': 1,\n",
    "    'model__min_samples_split': 10,\n",
    "    'model__n_estimators': 500,\n",
    "    'tfidf__max_df': 0.8416531114093017,\n",
    "    'tfidf__max_features': 10000,\n",
    "    'tfidf__min_df': 10,\n",
    "    'tfidf__ngram_range': (1, 1),\n",
    "    'tfidf__smooth_idf': True,\n",
    "    'tfidf__sublinear_tf': False,\n",
    "    'tfidf__use_idf': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_rfc = 0.7556111272907714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. <a id='toc4_5_'></a>–ú–æ–¥–µ–ª—å `LGBMClassifier`  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'tfidf__max_features':       hp.choice('tfidf__max_features', [None, 1000, 5000, 10000]),    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
    "    'tfidf__ngram_range':        hp.choice('tfidf__ngram_range', [(1, 1), (1, 2), (1, 3)]),      # –î–∏–∞–ø–∞–∑–æ–Ω n-–≥—Ä–∞–º–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "    'tfidf__min_df':             hp.choice('tfidf__min_df', [1, 2, 5]),                          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__max_df':             hp.uniform('tfidf__max_df', 0.7, 1.0),                          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Å–ª–æ–≤–æ\n",
    "    'tfidf__use_idf':            hp.choice('tfidf__use_idf', [True, False]),                     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ IDF\n",
    "    'tfidf__smooth_idf':         hp.choice('tfidf__smooth_idf', [True, False]),                  # –°–≥–ª–∞–∂–∏–≤–∞—Ç—å –ª–∏ IDF\n",
    "    'tfidf__sublinear_tf':       hp.choice('tfidf__sublinear_tf', [True, False]),                # –ü—Ä–∏–º–µ–Ω—è—Ç—å –ª–∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ TF\n",
    "\n",
    "    'model__boosting_type':      hp.choice('model__boosting_type', ['gbdt', 'dart', 'goss']),    # –¢–∏–ø –±—É—Å—Ç–∏–Ω–≥–∞\n",
    "    'model__num_leaves':         hp.quniform('model__num_leaves', 10, 200, 1),                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤ –≤ –¥–µ—Ä–µ–≤–µ\n",
    "    'model__learning_rate':      hp.loguniform('model__learning_rate', -5, 0),                   # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    'model__n_estimators':       hp.quniform('model__n_estimators', 50, 500, 1),                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "    'model__subsample':          hp.uniform('model__subsample', 0.5, 1.0),                       # –î–æ–ª—è –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞\n",
    "    'model__colsample_bytree':   hp.uniform('model__colsample_bytree', 0.5, 1.0),                # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞\n",
    "    'model__reg_alpha':          hp.loguniform('model__reg_alpha', -5, 2),                       # L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    'model__reg_lambda':         hp.loguniform('model__reg_lambda', -5, 2),                      # L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    'model__min_child_samples':  hp.quniform('model__min_child_samples', 5, 100, 1),             # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ\n",
    "    'model__max_depth':          hp.choice('model__max_depth', [-1, 3, 5, 7, 10]),               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (-1 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_lgbmc, best_score_lgbmc = get_model_results(lgbmc_pipeline, param_space, X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgbmc = {\n",
    "    'model__boosting_type': 'gbdt',\n",
    "    'model__colsample_bytree': 0.9998180861055651,\n",
    "    'model__learning_rate': 0.04363398114359794,\n",
    "    'model__max_depth': -1,\n",
    "    'model__min_child_samples': 9,\n",
    "    'model__n_estimators': 408,\n",
    "    'model__num_leaves': 107,\n",
    "    'model__reg_alpha': 0.0369531519316931,\n",
    "    'model__reg_lambda': 0.19369702791386537,\n",
    "    'model__subsample': 0.5376722197432499,\n",
    "    'tfidf__max_df': 0.9488532999515908,\n",
    "    'tfidf__max_features': None,\n",
    "    'tfidf__min_df': 2,\n",
    "    'tfidf__ngram_range': (1, 2),\n",
    "    'tfidf__smooth_idf': True,\n",
    "    'tfidf__sublinear_tf': False,\n",
    "    'tfidf__use_idf': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lgbmc = 0.773983000615775"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. <a id='toc4_6_'></a>–ú–æ–¥–µ–ª—å `CatBoostClassifier`  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_count = get_gpu_device_count()\n",
    "\n",
    "if gpu_count > 0:\n",
    "    task_type = 'GPU'\n",
    "    devices = '0'\n",
    "    print(colored('GPU –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω GPU.', 'green'))\n",
    "else:\n",
    "    task_type = 'CPU'\n",
    "    devices = None\n",
    "    print(colored('GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU.', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc = CatBoostClassifier(task_type=task_type,\n",
    "                         devices=devices,\n",
    "                         random_state=RANDOM_STATE,\n",
    "                         thread_count=-1,\n",
    "                         silent=True,\n",
    "                         text_features=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train.to_frame()\n",
    "X_test_df = X_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'learning_rate':       hp.uniform('learning_rate', 0.01, 0.3),    # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "    'depth':               hp.randint('depth', 6, 12),                # –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –∞–Ω—Å–∞–º–±–ª–µ.\n",
    "    'l2_leaf_reg':         hp.uniform('l2_leaf_reg', 1, 10),          # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è L2 –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 2),   # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –±–∞–≥–≥–∏–Ω–≥–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏.\n",
    "    'random_strength':     hp.uniform('random_strength', 0, 2),       # –°–∏–ª–∞ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ä–∞–∑–±–∏–µ–Ω–∏–π.\n",
    "    'subsample':           hp.uniform('subsample', 0.5, 1.0),         # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—É—Ç—Å—Ç—Ä—ç–ø–∞.\n",
    "    'colsample_bylevel':   hp.uniform('colsample_bylevel', 0.5, 1.0)  # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–µ—Ä–µ–≤–∞.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DillTrials(Trials):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DillTrials, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def _dump(self, trials_data):\n",
    "        return dill.dumps(trials_data)\n",
    "\n",
    "    def _load(self, dumped_trials_data):\n",
    "        return dill.loads(dumped_trials_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catboost_results(estimator,\n",
    "                         param_space: dict,\n",
    "                         X_train: pd.DataFrame,\n",
    "                         y_train: pd.Series,\n",
    "                         max_evals: int = 1000\n",
    "                         ) -> tuple[dict, float]:\n",
    "    objective = get_objective(estimator, X_train, y_train)\n",
    "    trials = DillTrials()\n",
    "\n",
    "    fmin(\n",
    "        fn=objective,\n",
    "        space=param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(RANDOM_STATE),\n",
    "        show_progressbar=True\n",
    "    )\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    best_params = trials.best_trial['result']['params']\n",
    "    best_score = abs(trials.best_trial['result']['loss'])\n",
    "    print(f'Finish with best F1 = {best_score:.4f}')\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_cbc, best_score_cbc = get_catboost_results(cbc, param_space, X_train_df, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_cbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. <a id='toc5_1_'></a>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=[best_score_rfc, best_score_cbc, best_score_lgbmc, best_score_lr],\n",
    "    index=['RandomForestClassifier', 'CatBoostClassifier', 'LGBMClassifier', 'LogisticRegression'],\n",
    "    columns=['f1']\n",
    ").sort_values('f1', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. <a id='toc5_2_'></a>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id='toc6_'></a>–û–±—â–∏–π –≤—ã–≤–æ–¥  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –º—ã —Ä–∞–±–æ—Ç–∞–ª–∏ —Å –∑–∞–¥–∞—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP), –∞ –∏–º–µ–Ω–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è).\n",
    "\n",
    "–ü–µ—Ä–µ–¥ –Ω–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ—Å—Ç–æ—è—â–∏–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–∑ `160 000` –∑–∞–ø–∏—Å–µ–π. –û–Ω [–¥–∞—Ç–∞—Å–µ—Ç] –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –æ—Ç–º–µ—Ç–∏–º –¥–∏–∑–±–∞–ª–∞–Ω—Å –ø—Ä–∏–º–µ—Ä–Ω–æ `9:1` –≤ —Å—Ç–æ—Ä–æ–Ω—É –Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (–∫–ª–∞—Å—Å `0`).\n",
    "\n",
    "–°–Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç –±—ã–ª –æ—á–∏—â–µ–Ω —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω.\n",
    "\n",
    "–ë—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã 4 —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ *hyperopt*, –∞ –∏–º–µ–Ω–Ω–æ: *LogisticRegression*, *RandomForestClassifier*, *LGBMClassifier* –∏ *CatBoostClassifier*. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–∏–∂–µ:\n",
    "\n",
    "|          Model         |   F1   |\n",
    "|:----------------------:|:------:|\n",
    "| LogisticRegression     | 0.6813 |\n",
    "| RandomForestClassifier | 0.7556 |\n",
    "| LGBMClassifier         | 0.7740 |\n",
    "| CatBoostClassifier     | 0.xxxx |\n",
    "\n",
    "–ò–∑ —ç—Ç–∏—Ö 4 –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª *CatBoostClassifier* —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º `F1 = 0.0000`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
