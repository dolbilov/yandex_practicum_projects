{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comments with BERT ü§¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω ¬´–í–∏–∫–∏—à–æ–ø¬ª –∑–∞–ø—É—Å–∫–∞–µ—Ç –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å. –¢–µ–ø–µ—Ä—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤, –∫–∞–∫ –≤ –≤–∏–∫–∏-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞—Ö. –¢–æ –µ—Å—Ç—å –∫–ª–∏–µ–Ω—Ç—ã –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç —Å–≤–æ–∏ –ø—Ä–∞–≤–∫–∏ –∏ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É—é—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö. –ú–∞–≥–∞–∑–∏–Ω—É –Ω—É–∂–µ–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–∫–∞—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∏—Ö –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é.\n",
    "\n",
    "**–¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞:** –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ.\n",
    "\n",
    "–¶–µ–ª–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞: `F1 >= 0.75`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ**<a id='toc0_'></a>    \n",
    "1. [ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞       ](#toc1_)    \n",
    "1.1. [–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏       ](#toc1_1_)    \n",
    "1.2. [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è       ](#toc1_2_)    \n",
    "1.3. [–°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏       ](#toc1_3_)    \n",
    "2. [–î–∞–Ω–Ω—ã–µ       ](#toc2_)    \n",
    "2.1. [–ó–∞–≥—Ä—É–∑–∫–∞       ](#toc2_1_)    \n",
    "2.2. [–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö       ](#toc2_2_)    \n",
    "2.3. [–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤       ](#toc2_3_)    \n",
    "2.4. [–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤       ](#toc2_4_)    \n",
    "2.5. [–ò–∑—É—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞       ](#toc2_5_)    \n",
    "2.6. [–í—ã–≤–æ–¥—ã –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É       ](#toc2_6_)    \n",
    "3. [–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞       ](#toc3_)    \n",
    "4. [–ú–æ–¥–µ–ª–∏       ](#toc4_)    \n",
    "4.1. [–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö       ](#toc4_1_)    \n",
    "4.2. [–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞      ](#toc4_2_)    \n",
    "4.3. [–ú–æ–¥–µ–ª—å `LogisticRegression`      ](#toc4_3_)    \n",
    "4.4. [–ú–æ–¥–µ–ª—å `RandomForestClassifier`     ](#toc4_4_)    \n",
    "4.5. [–ú–æ–¥–µ–ª—å `LGBMClassifier`     ](#toc4_5_)    \n",
    "4.6. [–ú–æ–¥–µ–ª—å `CatBoostClassifier`     ](#toc4_6_)    \n",
    "4.7. [–ú–æ–¥–µ–ª—å `BERT`  ](#toc4_7_)    \n",
    "4.8. [–ú–æ–¥–µ–ª—å `toxic-BERT` ](#toc4_8_)    \n",
    "5. [–†–µ–∑—É–ª—å—Ç–∞—Ç—ã     ](#toc5_)    \n",
    "5.1. [–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ     ](#toc5_1_)    \n",
    "5.2. [–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ     ](#toc5_2_)    \n",
    "6. [–û–±—â–∏–π –≤—ã–≤–æ–¥     ](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=true\n",
    "\tminLevel=2\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id='toc1_'></a> –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. <a id='toc1_1_'></a>–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q accelerate==0.26.0\n",
    "%pip install -q catboost==1.2.7\n",
    "%pip install -q dill==0.3.8\n",
    "%pip install -q hyperopt==0.2.7\n",
    "%pip install -q lightgbm==4.5.0\n",
    "%pip install -q matplotlib==3.8.4\n",
    "%pip install -q numpy==1.26.4\n",
    "%pip install -q optree==0.14.0\n",
    "%pip install -q pandas==2.2.3\n",
    "%pip install -q prettytable==3.12.0\n",
    "%pip install -q scikit-learn==1.5.2\n",
    "%pip install -q spacy==3.8.4\n",
    "%pip install -q termcolor==2.5.0\n",
    "%pip install -q torch==2.6.0\n",
    "%pip install -q torchvision==0.21.0\n",
    "%pip install -q tqdm==4.66.5\n",
    "%pip install -q transformers==4.49.0\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dill\n",
    "import torch\n",
    "import spacy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import (Dataset,\n",
    "                              DataLoader)\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from prettytable import PrettyTable\n",
    "from termcolor import colored\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     StratifiedKFold,\n",
    "                                     cross_val_score)\n",
    "from sklearn.metrics import (f1_score,\n",
    "                             classification_report,\n",
    "                             ConfusionMatrixDisplay)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from transformers import (BertTokenizer,\n",
    "                          BertModel,\n",
    "                          BertForSequenceClassification,\n",
    "                          AdamW)\n",
    "from hyperopt import (hp,\n",
    "                      fmin,\n",
    "                      tpe,\n",
    "                      Trials,\n",
    "                      STATUS_OK,\n",
    "                      STATUS_FAIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. <a id='toc1_2_'></a>–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 27\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. <a id='toc1_3_'></a>–°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(paths: list[str], **kwargs) -> pd.DataFrame:\n",
    "    for _path in paths:\n",
    "        if not exists(_path) and not _path.startswith('http'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(_path, **kwargs)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if df is None:\n",
    "            continue\n",
    "\n",
    "        return df\n",
    "\n",
    "    raise FileNotFoundError('No paths are valid for correct csv file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(df: pd.DataFrame) -> None:\n",
    "    duplicates_count = df.duplicated().sum()\n",
    "\n",
    "    if duplicates_count == 0:\n",
    "        print(colored('–ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.', 'green'))\n",
    "        return\n",
    "\n",
    "    duplicates_part = duplicates_count / len(df)\n",
    "    print(colored(f'–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {duplicates_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ ({duplicates_part:.2%})', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nans(df: pd.DataFrame) -> None:\n",
    "    if df.isna().sum().sum() == 0:\n",
    "        print(colored('–ü–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.', 'green'))\n",
    "        return\n",
    "\n",
    "    table = PrettyTable()\n",
    "    table.field_names = ['Feature', 'Missing values count']\n",
    "\n",
    "    missing_info = df.isna().sum().sort_values()\n",
    "    cols = missing_info.index.to_list()\n",
    "    for col in cols:\n",
    "        count = missing_info[col]\n",
    "        color = 'green' if count == 0 else 'red'\n",
    "        s = f'{count} ({count / len(df):.2%})'\n",
    "        table.add_row([col, colored(s, color)])\n",
    "\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_counts(series: pd.Series) -> None:\n",
    "    data = pd.DataFrame()\n",
    "    data['count'] = series.value_counts()\n",
    "    data['part'] = round(data['count'] / len(series), 4)\n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id='toc2_'></a>–î–∞–Ω–Ω—ã–µ        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. <a id='toc2_1_'></a>–ó–∞–≥—Ä—É–∑–∫–∞        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe([\n",
    "    './data/toxic_comments.csv',\n",
    "    'datasets/toxic_comments.csv',\n",
    "    'https://code.s3.yandex.net/datasets/toxic_comments.csv'\n",
    "], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. <a id='toc2_2_'></a>–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. <a id='toc2_3_'></a>–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. <a id='toc2_4_'></a>–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. <a id='toc2_5_'></a>–ò–∑—É—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_value_counts(df['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_counts = df['toxic'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(toxic_counts,\n",
    "        labels=['Not Toxic (0)', 'Toxic (1)'],\n",
    "        colors=['lightgreen', 'lightcoral'],\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∏–¥–∏–º –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–ª—å—à–æ–π –¥–∏–∑–±–∞–ª–∞–Ω—Å."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. <a id='toc2_6_'></a>–í—ã–≤–æ–¥—ã –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–¥ –Ω–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç –æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö —Å –æ—Ü–µ–Ω–∫–æ–π –∏—Ö —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏. –ü—Ä–æ–ø—É—Å–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–∞–∫–∂–µ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∫—Ä–∞–π–Ω–µ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ. –ó–∞–ø–∏—Å–µ–π —Å —Ç–∞—Ä–≥–µ—Ç–æ–º `0` (not toxic) –ø–æ—á—Ç–∏ –≤ 9 —Ä–∞–∑ –±–æ–ª—å—à–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id='toc3_'></a>–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"[^a-zA-Z\\s']\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_path = './data/lemmatized.csv'\n",
    "\n",
    "if exists(lemmatized_path):\n",
    "    df = pd.read_csv(lemmatized_path)\n",
    "else:\n",
    "    disabled_pipes = ['parser',  'ner']\n",
    "    nlp = spacy.load('en_core_web_md', disable=disabled_pipes)\n",
    "    texts = df['text'].apply(clean_text).tolist()\n",
    "    lemm_texts = []\n",
    "\n",
    "    for doc in tqdm(nlp.pipe(texts, disable=disabled_pipes), total=len(texts)):\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        lemm_text = ' '.join(lemmas)\n",
    "        lemm_texts.append(lemm_text)\n",
    "\n",
    "    df['text'] = lemm_texts\n",
    "\n",
    "    Path('./data').mkdir(exist_ok=True)\n",
    "    df.to_csv('./data/lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id='toc4_'></a>–ú–æ–¥–µ–ª–∏        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. <a id='toc4_1_'></a>–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö        [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=TEST_SIZE,\n",
    "                                                    random_state=RANDOM_STATE,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. <a id='toc4_2_'></a>–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞       [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objective(estimator,\n",
    "                  X_train: pd.DataFrame,\n",
    "                  y_train: pd.Series):\n",
    "    def objective(params: dict) -> float:\n",
    "        \"\"\"–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ç–µ–∫—É—â–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\n",
    "\n",
    "        Args:\n",
    "            estimator: –ø–∞–π–ø–ª–∞–π–Ω —Å –º–æ–¥–µ–ª—å—é –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω–æ –º–æ–¥–µ–ª—å\n",
    "            params (dict): –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "            X_train (pd.DataFrame): –≤—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (features)\n",
    "            y_train (pd.Series): —Ü–µ–ª–µ–≤–æ–π –ø—Ä–∏–∑–Ω–∞–∫ (target)\n",
    "\n",
    "        Returns:\n",
    "            dict: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –º–µ—Ç—Ä–∏–∫–∏, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∏ —Å—Ç–∞—Ç—É—Å–æ–º.\n",
    "        \"\"\"\n",
    "        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–µ, –ø—Ä–∏–≤–æ–¥–∏–º –∏—Ö –∫ –Ω—É–∂–Ω–æ–º—É —Ç–∏–ø—É\n",
    "        for key, value in params.items():\n",
    "            if isinstance(value, float) and value % 1 == 0:\n",
    "                params[key] = int(value)\n",
    "\n",
    "        estimator.set_params(**params)\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "        try:\n",
    "            score = cross_val_score(estimator=estimator,\n",
    "                                    X=X_train,\n",
    "                                    y=y_train,\n",
    "                                    scoring='f1',\n",
    "                                    cv=skf,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "            return {\n",
    "                'loss': -score.mean(),\n",
    "                'params': params,\n",
    "                'status': STATUS_OK\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return {'status': STATUS_FAIL}\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(estimator,\n",
    "                      param_space: dict,\n",
    "                      X_train: pd.DataFrame,\n",
    "                      y_train: pd.Series,\n",
    "                      max_evals: int = 1000\n",
    "                      ) -> tuple[dict, float]:\n",
    "    objective = get_objective(estimator, X_train, y_train)\n",
    "    trials = Trials()\n",
    "\n",
    "    fmin(\n",
    "        fn=objective,\n",
    "        space=param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(RANDOM_STATE),\n",
    "        show_progressbar=True\n",
    "    )\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    best_params = trials.best_trial['result']['params']\n",
    "    best_score = abs(trials.best_trial['result']['loss'])\n",
    "    print(f'Finish with best F1 = {best_score:.4f}')\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. <a id='toc4_3_'></a>–ú–æ–¥–µ–ª—å `LogisticRegression`       [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1))),\n",
    "    ('model', LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1, max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'tfidf__max_features':       hp.quniform('tfidf__max_features', 30_000, 50_000, 1000),       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ü–µ–ª—ã–µ —á–∏—Å–ª–∞)\n",
    "    'tfidf__min_df':             hp.choice('tfidf__min_df', [1, 2, 5, 10]),                      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__max_df':             hp.uniform('tfidf__max_df', 0.7, 1.0),                          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__use_idf':            hp.choice('tfidf__use_idf', [True, False]),                     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "    'tfidf__smooth_idf':         hp.choice('tfidf__smooth_idf', [True, False]),                  # –°–≥–ª–∞–∂–∏–≤–∞—Ç—å IDF –≤–µ—Å–∞\n",
    "    'tfidf__sublinear_tf':       hp.choice('tfidf__sublinear_tf', [True, False]),                # –ü—Ä–∏–º–µ–Ω—è—Ç—å —Å—É–±–ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ TF\n",
    "\n",
    "    'model__C':                  hp.loguniform('model__C', np.log(1e-3), np.log(15)),            # –û–±—Ä–∞—Ç–Ω–∞—è —Å–∏–ª–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (log-scale)\n",
    "    'model__class_weight':       hp.choice('model__class_weight', [None, 'balanced']),           # –í–µ—Å –∫–ª–∞—Å—Å–æ–≤ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–∞–Ω—Å –∏–ª–∏ –Ω–µ—Ç)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_lr, best_score_lr = get_model_results(lr_pipeline, param_space, X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lr = {\n",
    "    'model__C': 14.848553211901166,\n",
    "    'model__class_weight': None,\n",
    "    'tfidf__max_df': 0.7549417960132765,\n",
    "    'tfidf__max_features': 37000,\n",
    "    'tfidf__min_df': 5,\n",
    "    'tfidf__smooth_idf': True,\n",
    "    'tfidf__sublinear_tf': True,\n",
    "    'tfidf__use_idf': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lr = 0.7859790770261292"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. <a id='toc4_4_'></a>–ú–æ–¥–µ–ª—å `RandomForestClassifier`      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1))),\n",
    "    ('model', RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'tfidf__max_features':       hp.quniform('tfidf__max_features', 30_000, 50_000, 1000),   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ü–µ–ª—ã–µ —á–∏—Å–ª–∞)\n",
    "    'tfidf__min_df':             hp.choice('tfidf__min_df', [1, 2, 5, 10]),                  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__max_df':             hp.uniform('tfidf__max_df', 0.7, 1.0),                      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__use_idf':            hp.choice('tfidf__use_idf', [True, False]),                 # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "    'tfidf__smooth_idf':         hp.choice('tfidf__smooth_idf', [True, False]),              # –°–≥–ª–∞–∂–∏–≤–∞—Ç—å IDF –≤–µ—Å–∞\n",
    "    'tfidf__sublinear_tf':       hp.choice('tfidf__sublinear_tf', [True, False]),            # –ü—Ä–∏–º–µ–Ω—è—Ç—å —Å—É–±–ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ TF\n",
    "\n",
    "    'model__n_estimators':       hp.choice('model__n_estimators', [50, 100, 200, 500]),      # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É\n",
    "    'model__criterion':          hp.choice('model__criterion', ['gini', 'entropy']),         # –ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è\n",
    "    'model__max_depth':          hp.choice('model__max_depth', [None, 10, 20, 30, 50]),      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞\n",
    "    'model__min_samples_split':  hp.choice('model__min_samples_split', [2, 5, 10]),          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è\n",
    "    'model__min_samples_leaf':   hp.choice('model__min_samples_leaf', [1, 2, 4]),            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ\n",
    "    'model__bootstrap':          hp.choice('model__bootstrap', [True, False]),               # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±—É—Ç—Å—Ç—Ä—ç–ø –≤—ã–±–æ—Ä–∫–∏\n",
    "    'model__class_weight':       hp.choice('model__class_weight', [None, 'balanced']),       # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_rfc, best_score_rfc = get_model_results(rfc_pipeline, param_space, X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rfc = {\n",
    "    'model__bootstrap': True,\n",
    "    'model__class_weight': None,\n",
    "    'model__criterion': 'gini',\n",
    "    'model__max_depth': None,\n",
    "    'model__min_samples_leaf': 1,\n",
    "    'model__min_samples_split': 2,\n",
    "    'model__n_estimators': 200,\n",
    "    'tfidf__max_df': 0.7653612620932736,\n",
    "    'tfidf__max_features': 46000,\n",
    "    'tfidf__min_df': 10,\n",
    "    'tfidf__smooth_idf': True,\n",
    "    'tfidf__sublinear_tf': True,\n",
    "    'tfidf__use_idf': False\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_rfc = 0.7645131020400102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. <a id='toc4_5_'></a>–ú–æ–¥–µ–ª—å `LGBMClassifier`      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbmc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 1))),\n",
    "    ('model', LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, verbose=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'tfidf__max_features':       hp.quniform('tfidf__max_features', 30_000, 50_000, 1000),       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ü–µ–ª—ã–µ —á–∏—Å–ª–∞)\n",
    "    'tfidf__min_df':             hp.choice('tfidf__min_df', [1, 2, 5, 10]),                      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__max_df':             hp.uniform('tfidf__max_df', 0.7, 1.0),                          # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö\n",
    "    'tfidf__use_idf':            hp.choice('tfidf__use_idf', [True, False]),                     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "    'tfidf__smooth_idf':         hp.choice('tfidf__smooth_idf', [True, False]),                  # –°–≥–ª–∞–∂–∏–≤–∞—Ç—å IDF –≤–µ—Å–∞\n",
    "    'tfidf__sublinear_tf':       hp.choice('tfidf__sublinear_tf', [True, False]),                # –ü—Ä–∏–º–µ–Ω—è—Ç—å —Å—É–±–ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ TF\n",
    "\n",
    "    'model__boosting_type':      hp.choice('model__boosting_type', ['gbdt', 'dart', 'goss']),    # –¢–∏–ø –±—É—Å—Ç–∏–Ω–≥–∞\n",
    "    'model__num_leaves':         hp.quniform('model__num_leaves', 10, 200, 1),                   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç—å–µ–≤ –≤ –¥–µ—Ä–µ–≤–µ\n",
    "    'model__learning_rate':      hp.loguniform('model__learning_rate', -5, 0),                   # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    'model__n_estimators':       hp.quniform('model__n_estimators', 50, 500, 1),                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "    'model__subsample':          hp.uniform('model__subsample', 0.5, 1.0),                       # –î–æ–ª—è –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞\n",
    "    'model__colsample_bytree':   hp.uniform('model__colsample_bytree', 0.5, 1.0),                # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞\n",
    "    'model__reg_alpha':          hp.loguniform('model__reg_alpha', -5, 2),                       # L1-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    'model__reg_lambda':         hp.loguniform('model__reg_lambda', -5, 2),                      # L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è\n",
    "    'model__min_child_samples':  hp.quniform('model__min_child_samples', 5, 100, 1),             # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ\n",
    "    'model__max_depth':          hp.choice('model__max_depth', [-1, 3, 5, 7, 10]),               # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (-1 = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params_lgbmc, best_score_lgbmc = get_model_results(lgbmc_pipeline, param_space, X_train, y_train, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lgbmc = {\n",
    "    'model__boosting_type': 'gbdt',\n",
    "    'model__colsample_bytree': 0.6191104272221566,\n",
    "    'model__learning_rate': 0.09230661681722988,\n",
    "    'model__max_depth': -1,\n",
    "    'model__min_child_samples': 22,\n",
    "    'model__n_estimators': 298,\n",
    "    'model__num_leaves': 89,\n",
    "    'model__reg_alpha': 0.8456632205247746,\n",
    "    'model__reg_lambda': 0.12795690362150944,\n",
    "    'model__subsample': 0.5637403699663865,\n",
    "    'tfidf__max_df': 0.9783133591194336,\n",
    "    'tfidf__max_features': 34000,\n",
    "    'tfidf__min_df': 2,\n",
    "    'tfidf__smooth_idf': True,\n",
    "    'tfidf__sublinear_tf': True,\n",
    "    'tfidf__use_idf': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lgbmc = 0.7781002572293113"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. <a id='toc4_6_'></a>–ú–æ–¥–µ–ª—å `CatBoostClassifier`      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_count = get_gpu_device_count()\n",
    "\n",
    "if gpu_count > 0:\n",
    "    task_type = 'GPU'\n",
    "    devices = '0'\n",
    "    print(colored('GPU –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω GPU.', 'green'))\n",
    "else:\n",
    "    task_type = 'CPU'\n",
    "    devices = None\n",
    "    print(colored('GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU.', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc = CatBoostClassifier(task_type=task_type,\n",
    "                         devices=devices,\n",
    "                         random_state=RANDOM_STATE,\n",
    "                         silent=True,\n",
    "                         text_features=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = X_train.to_frame()\n",
    "X_test_df = X_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'learning_rate':        hp.uniform('learning_rate', 0.01, 0.3),    # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "    'depth':                hp.randint('depth', 6, 12),                # –ì–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –∞–Ω—Å–∞–º–±–ª–µ\n",
    "    'l2_leaf_reg':          hp.uniform('l2_leaf_reg', 1, 10),          # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è L2 –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "    'bagging_temperature':  hp.uniform('bagging_temperature', 0, 2),   # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –±–∞–≥–≥–∏–Ω–≥–∞ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏\n",
    "    'random_strength':      hp.uniform('random_strength', 0, 2),       # –°–∏–ª–∞ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ä–∞–∑–±–∏–µ–Ω–∏–π\n",
    "    'iterations':           hp.randint('iterations', 500, 1500),       # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –æ–±—É—á–µ–Ω–∏—è\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DillTrials(Trials):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(DillTrials, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def _dump(self, trials_data):\n",
    "        return dill.dumps(trials_data)\n",
    "\n",
    "    def _load(self, dumped_trials_data):\n",
    "        return dill.loads(dumped_trials_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_catboost_results(estimator,\n",
    "                         param_space: dict,\n",
    "                         X_train: pd.DataFrame,\n",
    "                         y_train: pd.Series,\n",
    "                         max_evals: int = 1000\n",
    "                         ) -> tuple[dict, float]:\n",
    "    objective = get_objective(estimator, X_train, y_train)\n",
    "    trials = DillTrials()\n",
    "\n",
    "    fmin(\n",
    "        fn=objective,\n",
    "        space=param_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(RANDOM_STATE),\n",
    "        show_progressbar=True\n",
    "    )\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    best_params = trials.best_trial['result']['params']\n",
    "    best_score = abs(trials.best_trial['result']['loss'])\n",
    "    print(f'Finish with best F1 = {best_score:.4f}')\n",
    "\n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_cbc, best_score_cbc = get_catboost_results(cbc, param_space, X_train_df, y_train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_params_cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_score_cbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. <a id='toc4_7_'></a>–ú–æ–¥–µ–ª—å `BERT`   [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–µ–Ω–∏–µ BERT –Ω–∞ –≤—Å–µ–º –∫–æ—Ä–ø—É—Å–µ –∑–∞–π–º–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏, –ø–æ—ç—Ç–æ–º—É —Å–æ–∫—Ä–∞—Ç–∏–º –¥–∞—Ç–∞—Å–µ—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_size = 1_000\n",
    "train_part = required_size / len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_bert, _, y_for_bert, _ = train_test_split(X_train,\n",
    "                                                y_train,\n",
    "                                                random_state=RANDOM_STATE,\n",
    "                                                train_size=train_part,\n",
    "                                                stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert, X_val_bert, y_train_bert, y_val_bert = train_test_split(X_for_bert,\n",
    "                                                                      y_for_bert,\n",
    "                                                                      random_state=RANDOM_STATE,\n",
    "                                                                      test_size=TEST_SIZE,\n",
    "                                                                      stratify=y_for_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_bert.shape, y_train_bert.shape)\n",
    "print(X_val_bert.shape, y_val_bert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CommentDataset(X_train_bert.values.tolist(), y_train_bert.values.tolist(), tokenizer)\n",
    "# val_dataset = CommentDataset(X_val_bert.values.tolist(), y_val_bert.values.tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_workers = 4 if torch.cuda.is_available() else 0\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=num_workers)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(colored('GPU –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω GPU.', 'green'))\n",
    "else:\n",
    "    print(colored('GPU –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU.', 'red'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# val_preds = []\n",
    "# val_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for batch in val_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "#         val_preds.extend(preds.cpu().numpy())\n",
    "#         val_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_bert = f1_score(val_labels, val_preds)\n",
    "# print(f'BERT F1 on val = {f1_bert:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay.from_predictions(val_labels, val_preds, cmap='Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~–ú–æ–¥–µ–ª—å BERT –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –∞–±—Å–æ–ª—é—Ç–Ω–æ –≤—Å–µ –∑–∞–ø–∏—Å–∏ –±—ã–ª–∏ –æ—Ç–Ω–µ—Å–µ–Ω—ã –∫ –∫–ª–∞—Å—Å—É `0`. –≠—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, —É—á–∏—Ç—ã–≤–∞—è –¥–∏–∑–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (`9:1`) –∏ –∫—Ä–∞–π–Ω–µ —Å–∏–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã undersampling'–∞ –∏ —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Å–∏–ª—å–Ω–æ –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤ (–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö). –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –º–æ–¥–µ–ª—å BERT **–ù–ï** –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. <a id='toc4_8_'></a>–ú–æ–¥–µ–ª—å `toxic-BERT`  [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts,\n",
    "                       return_tensors=\"pt\",\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É–ª–ª–∏–Ω–≥–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä (–≤–µ–∫—Ç–æ—Ä [CLS])\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = get_embeddings(X_train_bert.values.tolist())\n",
    "val_embeddings = get_embeddings(X_val_bert.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_lr = LogisticRegression(random_state=RANDOM_STATE, n_jobs=-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_lr.fit(train_embeddings, y_train_bert);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = toxic_lr.predict(val_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_toxic_bert = f1_score(y_val_bert, y_pred)\n",
    "print(f'F1 for toxic-BERT on val = {f1_toxic_bert:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_val_bert, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å toxic-BERT –ø–æ–∫–∞–∑–∞–ª–∞ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã. –ë—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –±—ç–∫–±–æ–Ω, –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –±—ã–ª–∞ –≤–∑—è—Ç–∞ LogisticRegression. –ü—Ä–∏ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –º–∞–ª–µ–Ω—å–∫–æ–π –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ (900 –∑–∞–ø–∏—Å–µ–π) —ç—Ç–∞ —Å–≤—è–∑–∫–∞ –ø–µ—Ä–µ–ø–ª—é–Ω—É–ª–∞ –≤—Å–µ –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –ø—Ä–∏–≤–µ–¥–µ—Ç –∫ –µ—â–µ –±–û–ª—å—à–µ–º—É —Ä–æ—Å—Ç—É –º–µ—Ç—Ä–∏–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Meme about Howard Stark that he's limited by the technology of his time\" src=\"./meme.jpg\" width=\"620\" height=\"414\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id='toc5_'></a>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. <a id='toc5_1_'></a>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=[best_score_rfc, 0, best_score_lgbmc, best_score_lr, 0, f1_toxic_bert],\n",
    "    index=['RandomForestClassifier', 'CatBoostClassifier', 'LGBMClassifier', 'LogisticRegression', 'BERT', 'toxic-BERT'],\n",
    "    columns=['f1']\n",
    ").sort_values('f1', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. <a id='toc5_2_'></a>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_size = 500\n",
    "train_part = required_size / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bert, _, y_test_bert, _ = train_test_split(X_test,\n",
    "                                                  y_test,\n",
    "                                                  random_state=RANDOM_STATE,\n",
    "                                                  train_size=train_part,\n",
    "                                                  stratify=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bert.shape, y_test_bert.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = get_embeddings(X_test_bert.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/ba/Warning_sign_4.0.png\" align=left width=44, heigth=33>\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>v2</b>  \n",
    "–¢—É—Ç —Å–æ–≤–µ—Ç –ø—Ä–æ—Å—Ç–æ–π - —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–∞—Ç—á–∞–º–∏.\n",
    "    \n",
    "–°–µ–π—á–∞—Å —Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–∞–µ—à—å –≤—Å–µ —Ç–µ–∫—Å—Ç—ã —Å—Ä–∞–∑—É, –Ω–æ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ –Ω–∞ GPU –±—É–¥–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ, –≤—Ä—è–¥–ª–∏ –º–æ–¥–µ–ª—å —Å–º–æ–∂–µ—Ç –≤–º–µ—Å—Ç–∏—Ç—å –≤ —Å–µ–±—è –±–æ–ª—å—à–µ 32-64 —Ç–µ–∫—Å—Ç–æ–≤ –∑–∞ —Ä–∞–∑.\n",
    "    \n",
    "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á–∏ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ –±–∞—Ç—á–∞–º.\n",
    "    \n",
    "–û–±—Ä–∞–∑–µ—Ü –∫–æ–¥–∞ —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –ø–æ –±–∞—Ç—á–∞–º –±—ã–ª –≤ —Å–ø—Ä–∏–Ω—Ç–µ.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bert = toxic_lr.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_test_bert, y_pred_bert)\n",
    "print(f'F1 on test = {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test_bert, y_pred_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test_bert, y_pred_bert, cmap='Greens');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id='toc6_'></a>–û–±—â–∏–π –≤—ã–≤–æ–¥      [&#8593;](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –º—ã —Ä–∞–±–æ—Ç–∞–ª–∏ —Å –∑–∞–¥–∞—á–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ (NLP), –∞ –∏–º–µ–Ω–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è).\n",
    "\n",
    "–ü–µ—Ä–µ–¥ –Ω–∞–º–∏ –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ—Å—Ç–æ—è—â–∏–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–∑ `160 000` –∑–∞–ø–∏—Å–µ–π. –û–Ω [–¥–∞—Ç–∞—Å–µ—Ç] –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –¢–∞–∫–∂–µ –æ—Ç–º–µ—Ç–∏–º –¥–∏–∑–±–∞–ª–∞–Ω—Å –ø—Ä–∏–º–µ—Ä–Ω–æ `9:1` –≤ —Å—Ç–æ—Ä–æ–Ω—É –Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (–∫–ª–∞—Å—Å `0`).\n",
    "\n",
    "–°–Ω–∞—á–∞–ª–∞ —Ç–µ–∫—Å—Ç –±—ã–ª –æ—á–∏—â–µ–Ω —Å –ø–æ–º–æ—â—å—é —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π, –∞ —Ç–∞–∫–∂–µ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω.\n",
    "\n",
    "–ë—ã–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã 4 —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ *hyperopt*, –∞ –∏–º–µ–Ω–Ω–æ: *LogisticRegression*, *RandomForestClassifier*, *LGBMClassifier* –∏ *CatBoostClassifier*. –î–ª—è –ø–µ—Ä–≤—ã—Ö —Ç—Ä—ë—Ö –º–æ–¥–µ–ª–µ–π –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω *TfidfVectorizer*, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ—Ç–æ—Ä–æ–≥–æ –ø–æ–¥–±–∏—Ä–∞–ª–∏—Å—å —Ç–∞–∫–∂–µ —á–µ—Ä–µ–∑ *hyperopt*. –ê –≤–æ—Ç *CatBoostClassifier* —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ \"–∏–∑ –∫–æ—Ä–æ–±–∫–∏\", —á—Ç–æ –∏ –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ.\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü–µ –Ω–∏–∂–µ:\n",
    "\n",
    "|          Model         |   F1   |\n",
    "|:----------------------:|:------:|\n",
    "| CatBoostClassifier     |   0    |\n",
    "| BERT                   |   0    |\n",
    "| RandomForestClassifier | 0.7645 |\n",
    "| LGBMClassifier         | 0.7781 |\n",
    "| LogisticRegression     | 0.7860 |\n",
    "| toxic-BERT             | 0.8235 |\n",
    "\n",
    "\n",
    "–ò–∑ —ç—Ç–∏—Ö 4 \"–∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö\" –º–æ–¥–µ–ª–µ–π –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å–µ–±—è –ø–æ–∫–∞–∑–∞–ª–∞ *LogisticRegression* —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º `F1 = 0.7860`.\n",
    "\n",
    "–¢–∞–∫–∂–µ –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è (bert-base-uncased) –º–æ–¥–µ–ª—å *BERT*. –ù–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ –¥–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ –Ω—É–ª–µ–≤—É—é F1, —Ç–∞–∫ –∫–∞–∫ –æ—Ç–Ω–µ—Å–ª–∞ –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∫ –∫–ª–∞—Å—Å—É `0`. –≠—Ç–æ –æ–∂–∏–¥–∞–µ–º–æ, —É—á–∏—Ç—ã–≤–∞—è –¥–∏–∑–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –∏ –∫—Ä–∞–π–Ω–µ —Å–∏–ª—å–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥—ã undersampling'–∞ –∏ —É–≤–µ–ª–∏—á–∏—Ç—å –æ–±—É—á–∞—é—â—É—é –≤—ã–±–æ—Ä–∫—É, —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Å–∏–ª—å–Ω–æ –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤ (–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö). –ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –º–æ–¥–µ–ª—å BERT **–ù–ï** –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.\n",
    "\n",
    "–ü–æ—Å–ª–µ–¥–Ω–µ–π –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å *toxic-BERT*. –û–¥–Ω–∞–∫–æ –µ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ–≤–æ–¥–∏–ª–æ—Å—å. –ë—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –±—ç–∫–±–æ–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ *LogisticRegression*. –î–∞–Ω–Ω–∞—è —Å–≤—è–∑–∫–∞ –ø–æ–∫–∞–∑–∞–ª–∞ –ø—Ä–µ–∫—Ä–∞—Å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –∫—Ä–∞–π–Ω–µ –º–∞–ª–æ–º –æ–±—É—á–∞—é—â–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ - `F1 = 0.8235`.\n",
    "\n",
    "–ò–º–µ–Ω–Ω–æ –æ–Ω–∞ –∏ –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ –∏ –ø–æ–∫–∞–∑–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç `F1 = 0.8571`. –≠—Ç–æ –±–æ–ª—å—à–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –º–µ—Ç—Ä–∏–∫–∏, —Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–µ–∫—Ç –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å —É—Å–ø–µ—à–Ω—ã–º. –î–ª—è —Ç–µ—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å –∫—Ä–∞–π–Ω–µ –º–∞–ª–∞—è —á–∞—Å—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ –≤–≤–∏–¥—É –±–æ–ª—å—à–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞—Ç—Ä–∞—Ç –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Å–º–µ—Ä—Ç–∏ —è–¥—Ä–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://s3.amazonaws.com/pix.iemoji.com/images/emoji/apple/ios-12/256/waving-hand.png\" align=left width=44, heigth=44>\n",
    "<div class=\"alert alert-info\">\n",
    "<b> —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–æ–ø. –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º</b>\n",
    "–ï—Å–ª–∏ —Ä–µ—à–∏—à—å –ø–æ–≥—Ä—É–∑–∏—Ç—å—Å—è –≤ –æ–±–ª–∞—Å—Ç—å —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–∞–º–∏, –æ—á–µ–Ω—å —Å–æ–≤–µ—Ç—É—é –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∫—É—Ä—Å–æ–≤.\n",
    "    \n",
    "   - –û—Ç–ª–∏—á–Ω—ã–π –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫—É—Ä—Å –æ—Ç –®–∫–æ–ª—ã –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ú–§–¢–ò (https://stepik.org/org/dlschool), —Å—Ç–∞—Ä—Ç –∫—É—Ä—Å–∞ –∫–∞–∂–¥—ã–µ –ø–æ–ª –≥–æ–¥–∞. –î–≤–∞ —Å–µ–º–µ—Å—Ç—Ä–∞, –æ–¥–∏–Ω –ø–æ –æ—Å–Ω–æ–≤–∞–º –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–º—É –∑—Ä–µ–Ω–∏—é, –≤—Ç–æ—Ä–æ–π –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ü—Ä–æ—Ö–æ–¥–∏—Ç—å –Ω—É–∂–Ω–æ –∏–º–µ–Ω–Ω–æ –≤ —Ç–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ,—Ç.–∫. –ø–æ—á—Ç–∏ –≤–µ—Å—å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π NLP –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –Ω–µ–π—Ä–æ–Ω–∫–∞—Ö.\n",
    "    \n",
    "   - \"–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ\" –æ—Ç Samsung Research Russia (https://stepik.org/course/50352/syllabus). –ï—Å—Ç—å —Ç–∞–∫–∂–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø–æ NLP.  \n",
    "   - –¢—Ä–µ–∫ NLP –æ—Ç —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ ODS https://ods.ai/tracks/nlp-course-autumn-22\n",
    "    \n",
    "    \n",
    "–ê –µ—Å–ª–∏ –Ω–∞ —Ç–µ–±—è –ø—Ä–æ–∏–∑–≤–µ–ª–∏ –≤–ø–µ—á–∞—Ç–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ ChatGPT –∏ —Ö–æ—á–µ—à—å –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–≤–æ–∏—Ö –∑–∞–¥–∞—á, –º–æ–≥—É –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ –∫—É—Ä—Å—ã (–ø–µ—Ä–≤—ã–µ –¥–≤–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –∑–∞—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–¥–±–æ—Ä–∫–∞ –ª–µ–∫—Ü–∏–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º).\n",
    "    \n",
    "  - https://www.coursera.org/learn/generative-ai-with-llms  (–º–æ–∂–Ω–æ –ø—Ä–æ—Å–ª—É–∂–∞—Ç—å –±–µ—Å–ø–ª–∞—Ç–Ω–æ)\n",
    "  - –ö–æ—Ä–æ—Ç–∫–∏–µ –∫—É—Ä—Å—ã –Ω–∞ —Å–∞–π—Ç–µ https://www.deeplearning.ai/short-courses/  –°–∞–º—ã–π —Å–≤–µ–∂–∞–∫ - –∫–∞–∫ –ø–∏—Å–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä –∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å –±–æ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ ChatGPT, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–∏).  \n",
    "  - https://www.promptingguide.ai/introduction/settings\n",
    "  - https://www.youtube.com/watch?v=l-l82uNwyu8&list=PLy6K3_Hx-udj6n1S88Vslyw2QVxSXLP2c    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Ç—É—Ç–æ—Ä–∏–∞–ª—É –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ HugingFace. –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ, –¥–ª—è –±–æ–ª—å—à–µ —á–µ–º –ø–æ–ª–æ–≤–∏–Ω—ã –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ —Ä—É—Å—Å–∫–∏–π, —Ö–æ—Ç—è –º–æ–∂–Ω–æ —á–∏—Ç–∞—Ç—å –∏ –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞. \n",
    "    \n",
    "  https://huggingface.co/learn/nlp-course   \n",
    " \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
